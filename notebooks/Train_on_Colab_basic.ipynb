{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "044f1b27",
   "metadata": {},
   "source": [
    "# Train PhoBERT for Vietnamese COVID-19 NER on Google Colab\n",
    "\n",
    "This notebook is designed to run the training process for the Vietnamese COVID-19 NER task on Google Colab. It combines the configuration, dataset, and training scripts into a single file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5ec69",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "First, need to install the required Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8557caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers seqeval pandas torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcac999b",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive and Prepare Data\n",
    "\n",
    "Mount your Google Drive to access the dataset. You will need to upload your `data` folder to your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e52630e",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "This cell contains all the configurations and hyperparameters for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf9abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- 1. Paths ---\n",
    "# IMPORTANT: Update this path to point to your project folder on Google Drive\n",
    "BASE_PROJECT_DIR = '/content/drive/MyDrive/'\n",
    "DATA_DIR = os.path.join(BASE_PROJECT_DIR, 'raw/PhoNER_COVID19/')\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, 'train_word.json')\n",
    "DEV_FILE = os.path.join(DATA_DIR, 'dev_word.json')\n",
    "TEST_FILE = os.path.join(DATA_DIR, 'test_word.json')\n",
    "\n",
    "# Thư mục để lưu các mô hình đã huấn luyện\n",
    "MODEL_OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'models/phobert-ner-covid')\n",
    "\n",
    "\n",
    "# --- 2. Cấu hình Mô hình (Model Configuration) ---\n",
    "PRE_TRAINED_MODEL_NAME = \"vinai/phobert-base\"\n",
    "\n",
    "\n",
    "# --- 3. Siêu tham số Huấn luyện (Training Hyperparameters) ---\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 3e-5\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "# --- 4. Tag Configuration ---\n",
    "UNIQUE_TAGS = [\n",
    "    'O', 'B-AGE', 'I-AGE', 'B-DATE', 'I-DATE', 'B-GENDER', 'B-JOB', 'I-JOB',\n",
    "    'B-LOCATION', 'I-LOCATION', 'B-NAME', 'I-NAME', 'B-ORGANIZATION', 'I-ORGANIZATION',\n",
    "    'B-PATIENT_ID', 'I-PATIENT_ID', 'B-SYMPTOM_AND_DISEASE', 'I-SYMPTOM_AND_DISEASE',\n",
    "    'B-TRANSPORTATION', 'I-TRANSPORTATION'\n",
    "]\n",
    "\n",
    "TAGS_TO_IDS = {tag: i for i, tag in enumerate(UNIQUE_TAGS)}\n",
    "IDS_TO_TAGS = {i: tag for i, tag in enumerate(UNIQUE_TAGS)}\n",
    "SUBWORD_TAG_ID = -100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185be9b9",
   "metadata": {},
   "source": [
    "## 4. Dataset Class\n",
    "\n",
    "This cell defines the `NerDataset` class, which is responsible for loading, preprocessing, and preparing the data for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0d6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "class NerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_len, tags_to_ids):\n",
    "        self.file_path = file_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.tags_to_ids = tags_to_ids\n",
    "        self.subword_tag_id = -100\n",
    "        self.sentences, self.tags = self._read_data()\n",
    "\n",
    "    def _read_data(self):\n",
    "        if not os.path.exists(self.file_path):\n",
    "            raise FileNotFoundError(f\"File not found at: {self.file_path}\")\n",
    "        df = pd.read_json(self.file_path, lines=True, encoding='utf-8')\n",
    "        sentences = df['words'].tolist()\n",
    "        tags = df['tags'].tolist()\n",
    "        return sentences, tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        words = self.sentences[index]\n",
    "        tags = self.tags[index]\n",
    "        input_ids = []\n",
    "        target_tags = []\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            word_tokens = self.tokenizer.tokenize(word)\n",
    "            if len(word_tokens) > 0:\n",
    "                input_ids.extend(self.tokenizer.convert_tokens_to_ids(word_tokens))\n",
    "                tag_id = self.tags_to_ids.get(tags[i], self.tags_to_ids['O'])\n",
    "                target_tags.append(tag_id)\n",
    "                target_tags.extend([self.subword_tag_id] * (len(word_tokens) - 1))\n",
    "\n",
    "        if len(input_ids) > self.max_len - 2:\n",
    "            input_ids = input_ids[:self.max_len - 2]\n",
    "            target_tags = target_tags[:self.max_len - 2]\n",
    "\n",
    "        final_input_ids = [self.tokenizer.cls_token_id] + input_ids + [self.tokenizer.sep_token_id]\n",
    "        final_target_tags = [self.subword_tag_id] + target_tags + [self.subword_tag_id]\n",
    "        attention_mask = [1] * len(final_input_ids)\n",
    "\n",
    "        padding_length = self.max_len - len(final_input_ids)\n",
    "        final_input_ids = final_input_ids + ([self.tokenizer.pad_token_id] * padding_length)\n",
    "        attention_mask = attention_mask + ([0] * padding_length)\n",
    "        final_target_tags = final_target_tags + ([self.subword_tag_id] * padding_length)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(final_input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(final_target_tags, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37819be4",
   "metadata": {},
   "source": [
    "## 5. Training and Evaluation Functions\n",
    "\n",
    "This cell contains the core logic for training and evaluating the NER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e636d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device, ids_to_tags):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            true_labels = labels.cpu().numpy()\n",
    "\n",
    "            for i in range(len(true_labels)):\n",
    "                pred_tags = [ids_to_tags[p] for p, l in zip(predictions[i], true_labels[i]) if l != SUBWORD_TAG_ID]\n",
    "                label_tags = [ids_to_tags[l] for l in true_labels[i] if l != SUBWORD_TAG_ID]\n",
    "                all_preds.append(pred_tags)\n",
    "                all_labels.append(label_tags)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "\n",
    "    return avg_loss, f1, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a278456d",
   "metadata": {},
   "source": [
    "## 6. Run Training\n",
    "\n",
    "This is the main function to run the entire training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fe6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    set_seed(RANDOM_SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, use_fast=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        PRE_TRAINED_MODEL_NAME,\n",
    "        num_labels=len(UNIQUE_TAGS),\n",
    "        id2label=IDS_TO_TAGS,\n",
    "        label2id=TAGS_TO_IDS\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    train_dataset = NerDataset(\n",
    "        file_path=TRAIN_FILE,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN,\n",
    "        tags_to_ids=TAGS_TO_IDS\n",
    "    )\n",
    "    dev_dataset = NerDataset(\n",
    "        file_path=DEV_FILE,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN,\n",
    "        tags_to_ids=TAGS_TO_IDS\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=VALID_BATCH_SIZE)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    num_training_steps = len(train_dataloader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    best_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch + 1}/{EPOCHS} ---\")\n",
    "        train_loss = train_one_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        val_loss, val_f1, val_precision, val_recall = evaluate(model, dev_dataloader, device, IDS_TO_TAGS)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation F1: {val_f1:.4f} | Precision: {val_precision:.4f} | Recall: {val_recall:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            print(f\"New best F1 score: {best_f1:.4f}. Saving model...\")\n",
    "            model.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "            tokenizer.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "    \n",
    "    print(\"\\nTraining finished!\")\n",
    "    print(f\"Best F1 score on validation set: {best_f1:.4f}\")\n",
    "    print(f\"Model saved to {MODEL_OUTPUT_DIR}\")\n",
    "\n",
    "run_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
