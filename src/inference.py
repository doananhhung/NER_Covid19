
import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification
import numpy as np
import sys
import os
import re
from typing import List, Dict

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import config
from text_processor import get_text_processor

class NERPredictor:
    """
    L·ªõp ƒë√≥ng g√≥i m√¥ h√¨nh NER ƒë·ªÉ th·ª±c hi·ªán d·ª± ƒëo√°n tr√™n vƒÉn b·∫£n m·ªõi.
    """
    def __init__(self, model_path: str, use_word_segmentation: bool = True):
        """
        H√†m kh·ªüi t·∫°o.

        Args:
            model_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a m√¥ h√¨nh v√† tokenizer ƒë√£ l∆∞u.
            use_word_segmentation (bool): S·ª≠ d·ª•ng word segmentation hay kh√¥ng (m·∫∑c ƒë·ªãnh True).
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model = AutoModelForTokenClassification.from_pretrained(model_path)
            self.model.to(self.device)
            self.model.eval()
            
            self.ids_to_tags = self.model.config.id2label
            print(f"Model loaded successfully from {model_path} on device {self.device}")
        except OSError:
            print(f"L·ªói: Kh√¥ng t√¨m th·∫•y model t·∫°i '{model_path}'.")
            self.model = None
        
        self.use_word_segmentation = use_word_segmentation
        self.text_processor = get_text_processor() if use_word_segmentation else None
        
        if use_word_segmentation:
            if self.text_processor and self.text_processor.is_available():
                print("VnCoreNLP word segmentation ƒë√£ s·∫µn s√†ng")
            else:
                print("C·∫£nh b√°o: VnCoreNLP kh√¥ng kh·∫£ d·ª•ng. Word segmentation s·∫Ω b·ªã v√¥ hi·ªáu h√≥a.")
                self.use_word_segmentation = False
    
    def segment_text(self, text: str) -> str:
        """
        T√°ch t·ª´ ti·∫øng Vi·ªát s·ª≠ d·ª•ng VnCoreNLP.
        
        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o.
            
        Returns:
            str: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c t√°ch t·ª´ (c√°c t·ª´ gh√©p n·ªëi b·∫±ng d·∫•u _)
        """
        if not self.use_word_segmentation or not self.text_processor:
            return text
        
        segmented = self.text_processor.segment_text(text)
        return segmented if segmented is not None else text

    def predict(self, sentence: str, max_length: int = 220, show_debug: bool = False):
        """
        D·ª± ƒëo√°n c√°c th·ª±c th·ªÉ trong m·ªôt c√¢u. T·ª± ƒë·ªông x·ª≠ l√Ω vƒÉn b·∫£n d√†i h∆°n gi·ªõi h·∫°n c·ªßa model.

        Args:
            sentence (str): C√¢u vƒÉn b·∫£n ƒë·∫ßu v√†o (ch∆∞a segment).
            max_length (int): ƒê·ªô d√†i t·ªëi ƒëa c·ªßa m·ªói chunk (m·∫∑c ƒë·ªãnh 220 tokens).
            show_debug (bool): Hi·ªÉn th·ªã th√¥ng tin debug hay kh√¥ng.

        Returns:
            list: M·ªôt danh s√°ch c√°c dictionary, m·ªói dictionary ch·ª©a th√¥ng tin v·ªÅ m·ªôt th·ª±c th·ªÉ (text, tag, start, end).
        """
        if not self.model:
            print("Model ch∆∞a ƒë∆∞·ª£c t·∫£i. Kh√¥ng th·ªÉ d·ª± ƒëo√°n.")
            return []
        
        # KI·ªÇM TRA VƒÇN B·∫¢N ƒê·∫¶U V√ÄO
        print(f"\n{'='*80}")
        print(f" VƒÇN B·∫¢N ƒê·∫¶U V√ÄO:")
        print(f"   - ƒê·ªô d√†i: {len(sentence)} k√Ω t·ª±")
        print(f"   - S·ªë t·ª´: {len(sentence.split())} t·ª´")
        print(f"   - 100 k√Ω t·ª± ƒë·∫ßu: {sentence[:100]}...")
        print(f"   - 100 k√Ω t·ª± cu·ªëi: ...{sentence[-100:]}")
        print(f"{'='*80}\n")
        
        # L∆∞u vƒÉn b·∫£n g·ªëc (ch∆∞a segment) ƒë·ªÉ t√¨m v·ªã tr√≠ entities
        original_text = sentence
        
        # √Åp d·ª•ng word segmentation n·∫øu ƒë∆∞·ª£c b·∫≠t
        if self.use_word_segmentation:
            if show_debug:
                print(f" Original text: {sentence[:100]}...")
            sentence_segmented = self.segment_text(sentence)
            if show_debug:
                print(f" Segmented text: {sentence_segmented[:100]}...")
        else:
            sentence_segmented = sentence

        # Ki·ªÉm tra ƒë·ªô d√†i vƒÉn b·∫£n
        tokens = self.tokenizer.tokenize(sentence_segmented)
        
        print(f"\n Th√¥ng tin x·ª≠ l√Ω:")
        print(f"   - ƒê·ªô d√†i vƒÉn b·∫£n g·ªëc: {len(original_text)} k√Ω t·ª±")
        print(f"   - ƒê·ªô d√†i vƒÉn b·∫£n ƒë√£ segment: {len(sentence_segmented)} k√Ω t·ª±")
        print(f"   - S·ªë tokens: {len(tokens)}")
        print(f"   - Max length: {max_length}")
        
        if len(tokens) <= max_length:
            # VƒÉn b·∫£n ng·∫Øn - predict tr·ª±c ti·∫øp
            print(f"    X·ª≠ l√Ω tr·ª±c ti·∫øp (vƒÉn b·∫£n ng·∫Øn)")
            return self._predict_single(sentence_segmented, show_debug=show_debug, 
                                       original_text=original_text, text_offset=0)
        else:
            # VƒÉn b·∫£n d√†i - chia nh·ªè v√† predict
            print(f"     Chia th√†nh chunks (vƒÉn b·∫£n d√†i: {len(tokens)} > {max_length} tokens)")
            return self._predict_long_text(sentence_segmented, max_length, show_debug=show_debug,
                                          original_text=original_text)

    def _predict_single(self, sentence: str, show_debug: bool = False, original_text: str = None, text_offset: int = 0):
        """
        D·ª± ƒëo√°n c√°c th·ª±c th·ªÉ cho vƒÉn b·∫£n ng·∫Øn (‚â§ max_length tokens).

        Args:
            sentence (str): C√¢u vƒÉn b·∫£n ƒë·∫ßu v√†o.
            show_debug (bool): Hi·ªÉn th·ªã th√¥ng tin debug hay kh√¥ng.
            original_text (str): VƒÉn b·∫£n g·ªëc ƒë·ªÉ t√¨m v·ªã tr√≠ ch√≠nh x√°c (cho vƒÉn b·∫£n d√†i).
            text_offset (int): Offset c·ªßa sentence trong original_text.

        Returns:
            list: Danh s√°ch c√°c entity v·ªõi th√¥ng tin text, tag, start, end.
        """
        # 1. Tokenization - KH√îNG TRUNCATE ƒë·ªÉ tr√°nh m·∫•t d·ªØ li·ªáu
        encoding = self.tokenizer(
            sentence, 
            return_tensors="pt",
            truncation=False,  # QUAN TR·ªåNG: Kh√¥ng c·∫Øt vƒÉn b·∫£n
            padding=False
        )
        input_ids = encoding["input_ids"].to(self.device)
        attention_mask = encoding["attention_mask"].to(self.device)
        
        # Ki·ªÉm tra ƒë·ªô d√†i v√† c·∫£nh b√°o n·∫øu qu√° d√†i
        actual_length = input_ids.shape[1]
        if actual_length > 256:
            print(f"\n  C·∫¢NH B√ÅO: Sentence c√≥ {actual_length} tokens, v∆∞·ª£t qu√° max_length c·ªßa model (256)!")
            print(f"   ƒêang C·∫ÆT B·ªé ph·∫ßn c√≤n l·∫°i. H√£y d√πng _predict_long_text() thay th·∫ø!")
            # Truncate th·ªß c√¥ng
            input_ids = input_ids[:, :256]
            attention_mask = attention_mask[:, :256]

        # 2. Inference
        with torch.no_grad():
            outputs = self.model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits

        predictions = torch.argmax(logits, dim=2)[0].cpu().numpy()
        
        # L·∫•y c√°c token v√† d·ª± ƒëo√°n t∆∞∆°ng ·ª©ng
        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())
        predicted_tags = [self.ids_to_tags[p] for p in predictions]
        
        # Debug: In ra tokens v√† predicted tags ƒë·ªÉ ki·ªÉm tra
        if show_debug:
            print("\n=== DEBUG INFO ===")
            print(f"Sentence: {sentence}")
            print(f"Number of tokens: {len(tokens)}")
            print("Tokens and Tags:")
            for i, (token, tag) in enumerate(zip(tokens, predicted_tags)):
                print(f"  {i}: '{token}' -> {tag}")
            print("==================\n")

        # 3. Post-processing: Nh√≥m c√°c sub-word token th√†nh th·ª±c th·ªÉ ho√†n ch·ªânh
        # X·ª≠ l√Ω ƒë√∫ng v·ªõi PhoBERT BPE tokenizer (@@)
        entities = []
        current_entity_tokens = []
        current_entity_token_indices = []  # L∆∞u v·ªã tr√≠ c·ªßa c√°c token
        current_entity_tag = ""

        def merge_tokens_to_text(token_list):
            """
            Gh√©p c√°c BPE tokens th√†nh text g·ªëc s·ª≠ d·ª•ng tokenizer.
            ƒê√¢y l√† c√°ch ƒê√öNG ƒë·ªÉ x·ª≠ l√Ω PhoBERT tokens.
            """
            if not token_list:
                return ""
            
            # S·ª≠ d·ª•ng tokenizer ƒë·ªÉ convert ch√≠nh x√°c
            # T·∫°o m·ªôt chu·ªói gi·∫£ ƒë·ªÉ tokenizer x·ª≠ l√Ω
            text = self.tokenizer.convert_tokens_to_string(token_list)
            
            return text.strip()

        # B·ªè qua token [CLS] ·ªü ƒë·∫ßu v√† [SEP] ·ªü cu·ªëi
        for i in range(1, len(predicted_tags) - 1):
            token = tokens[i]
            tag = predicted_tags[i]
            
            # Ki·ªÉm tra xem token hi·ªán t·∫°i c√≥ ph·∫£i l√† ph·∫ßn ti·∫øp theo c·ªßa BPE kh√¥ng
            # PhoBERT BPE: token tr∆∞·ªõc k·∫øt th√∫c b·∫±ng @@ th√¨ token hi·ªán t·∫°i l√† ph·∫ßn ti·∫øp theo
            is_bpe_continuation = (i > 1 and tokens[i-1].endswith("@@"))

            if tag.startswith("B-"):
                # TR∆Ø·ªúNG H·ª¢P ƒê·∫∂C BI·ªÜT: N·∫øu token n√†y l√† ph·∫ßn ti·∫øp theo c·ªßa BPE 
                # v√† ƒëang c√≥ entity, th√¨ ∆∞u ti√™n merge v√†o entity hi·ªán t·∫°i
                if is_bpe_continuation and current_entity_tokens:
                    current_entity_tokens.append(token)
                    current_entity_token_indices.append(i)
                    if show_debug:
                        print(f"  [BPE-FIX] Merged '{token}' (B-{tag[2:]}) into current entity '{current_entity_tag}' due to BPE continuation")
                else:
                    # N·∫øu ƒëang c√≥ m·ªôt th·ª±c th·ªÉ, l∆∞u n√≥ l·∫°i tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu th·ª±c th·ªÉ m·ªõi
                    if current_entity_tokens:
                        entity_text = merge_tokens_to_text(current_entity_tokens)
                        if entity_text:
                            entities.append({
                                "text": entity_text, 
                                "tag": current_entity_tag,
                                "token_indices": current_entity_token_indices.copy()
                            })
                    
                    # B·∫Øt ƒë·∫ßu m·ªôt th·ª±c th·ªÉ m·ªõi
                    current_entity_tokens = [token]
                    current_entity_token_indices = [i]
                    current_entity_tag = tag[2:]  # L·∫•y t√™n tag (v√≠ d·ª•: LOCATION t·ª´ B-LOCATION)
            
            elif tag.startswith("I-"):
                # TR∆Ø·ªúNG H·ª¢P 1: Kh·ªõp v·ªõi tag hi·ªán t·∫°i
                if current_entity_tag == tag[2:]:
                    current_entity_tokens.append(token)
                    current_entity_token_indices.append(i)
                # TR∆Ø·ªúNG H·ª¢P 2: Token n√†y l√† ph·∫ßn ti·∫øp theo c·ªßa BPE
                elif is_bpe_continuation and current_entity_tokens:
                    # Th√™m v√†o entity hi·ªán t·∫°i ngay c·∫£ khi tag kh√¥ng kh·ªõp (s·ª≠a l·ªói m√¥ h√¨nh)
                    current_entity_tokens.append(token)
                    current_entity_token_indices.append(i)
                    if show_debug:
                        print(f"  [BPE-FIX] Merged '{token}' (I-{tag[2:]}) into current entity '{current_entity_tag}' due to BPE continuation")
                else:
                    # Tag kh√¥ng kh·ªõp v√† kh√¥ng ph·∫£i ph·∫ßn ti·∫øp theo, l∆∞u entity c≈©
                    if current_entity_tokens:
                        entity_text = merge_tokens_to_text(current_entity_tokens)
                        if entity_text:
                            entities.append({
                                "text": entity_text, 
                                "tag": current_entity_tag,
                                "token_indices": current_entity_token_indices.copy()
                            })
                    # B·∫Øt ƒë·∫ßu entity m·ªõi v·ªõi tag I- n√†y (x·ª≠ l√Ω tr∆∞·ªùng h·ª£p thi·∫øu B-)
                    current_entity_tokens = [token]
                    current_entity_token_indices = [i]
                    current_entity_tag = tag[2:]
            
            else: # Tag l√† 'O'
                # TR∆Ø·ªúNG H·ª¢P ƒê·∫∂C BI·ªÜT: N·∫øu token n√†y l√† ph·∫ßn ti·∫øp theo c·ªßa BPE v√† ƒëang trong entity
                # th√¨ ∆∞u ti√™n merge v√†o entity (v√¨ model c√≥ th·ªÉ tag sai)
                if is_bpe_continuation and current_entity_tokens:
                    current_entity_tokens.append(token)
                    current_entity_token_indices.append(i)
                    if show_debug:
                        print(f"  [BPE-FIX] Merged '{token}' (O) into current entity '{current_entity_tag}' due to BPE continuation")
                else:
                    # N·∫øu ƒëang c√≥ m·ªôt th·ª±c th·ªÉ, l∆∞u n√≥ l·∫°i
                    if current_entity_tokens:
                        entity_text = merge_tokens_to_text(current_entity_tokens)
                        if entity_text:
                            entities.append({
                                "text": entity_text, 
                                "tag": current_entity_tag,
                                "token_indices": current_entity_token_indices.copy()
                            })
                    
                    # Reset
                    current_entity_tokens = []
                    current_entity_token_indices = []
                    current_entity_tag = ""
        
        # L∆∞u l·∫°i th·ª±c th·ªÉ cu·ªëi c√πng n·∫øu n√≥ k√©o d√†i ƒë·∫øn h·∫øt c√¢u
        if current_entity_tokens:
            entity_text = merge_tokens_to_text(current_entity_tokens)
            if entity_text:
                entities.append({
                    "text": entity_text, 
                    "tag": current_entity_tag,
                    "token_indices": current_entity_token_indices.copy()
                })

        # Th√™m th√¥ng tin v·ªã tr√≠ (start, end) cho m·ªói entity b·∫±ng c√°ch t√¨m trong c√¢u g·ªëc
        entities_with_positions = []
        used_positions = []  # Theo d√µi v·ªã tr√≠ ƒë√£ s·ª≠ d·ª•ng ƒë·ªÉ tr√°nh tr√πng l·∫∑p
        
        # X√°c ƒë·ªãnh text ƒë·ªÉ search
        search_text = original_text if original_text else sentence
        base_offset = text_offset if original_text else 0
        
        for entity in entities:
            entity_text = entity["text"]
            entity_tag = entity["tag"]
            
            # Convert entity_text t·ª´ d·∫°ng segmented v·ªÅ d·∫°ng original
            # V√≠ d·ª•: "Trung_t√¢m" -> "Trung t√¢m"
            entity_text_original = entity_text.replace('_', ' ')
            
            # Chi·∫øn l∆∞·ª£c t√¨m ki·∫øm entity trong text g·ªëc:
            # 1. T√¨m trong v√πng hi·ªán t·∫°i (sentence) v·ªõi offset
            # 2. M·ªü r·ªông v√πng t√¨m ki·∫øm n·∫øu kh√¥ng th·∫•y
            
            # B∆∞·ªõc 1: T√¨m trong v√πng g·∫ßn text_offset (∆∞u ti√™n)
            search_start = max(0, base_offset)
            search_end = min(len(search_text), base_offset + len(sentence) + 100)
            search_region = search_text[search_start:search_end]
            
            found = False
            # T√¨m t·∫•t c·∫£ c√°c v·ªã tr√≠ xu·∫•t hi·ªán c·ªßa entity_text_original trong search_region
            search_pos = 0
            while True:
                pos = search_region.find(entity_text_original, search_pos)
                if pos == -1:
                    break
                
                # T√≠nh v·ªã tr√≠ tuy·ªát ƒë·ªëi
                absolute_pos = search_start + pos
                absolute_end = absolute_pos + len(entity_text_original)
                
                # Ki·ªÉm tra xem v·ªã tr√≠ n√†y ƒë√£ ƒë∆∞·ª£c s·ª≠ d·ª•ng ch∆∞a
                is_used = any(absolute_pos < used_end and absolute_end > used_start 
                             for used_start, used_end in used_positions)
                
                if not is_used:
                    entities_with_positions.append({
                        "text": entity_text_original,  # S·ª≠ d·ª•ng text g·ªëc (kh√¥ng c√≥ _)
                        "tag": entity_tag,
                        "start": absolute_pos,
                        "end": absolute_end
                    })
                    used_positions.append((absolute_pos, absolute_end))
                    found = True
                    break
                
                # Ti·∫øp t·ª•c t√¨m ·ªü v·ªã tr√≠ ti·∫øp theo
                search_pos = pos + 1
            
            if not found:
                # B∆∞·ªõc 2: M·ªü r·ªông t√¨m ki·∫øm trong to√†n b·ªô vƒÉn b·∫£n
                search_pos = 0
                while True:
                    pos = search_text.find(entity_text_original, search_pos)
                    if pos == -1:
                        break
                    
                    absolute_end = pos + len(entity_text_original)
                    is_used = any(pos < used_end and absolute_end > used_start 
                                 for used_start, used_end in used_positions)
                    
                    if not is_used:
                        entities_with_positions.append({
                            "text": entity_text_original,  # S·ª≠ d·ª•ng text g·ªëc (kh√¥ng c√≥ _)
                            "tag": entity_tag,
                            "start": pos,
                            "end": absolute_end
                        })
                        used_positions.append((pos, absolute_end))
                        found = True
                        break
                    
                    search_pos = pos + 1
            
            if not found:
                # V·∫´n kh√¥ng t√¨m th·∫•y - th√™m v√†o nh∆∞ng kh√¥ng c√≥ v·ªã tr√≠
                if show_debug:
                    print(f"    Could not find position for entity: '{entity_text_original}' ({entity_tag})")
                entities_with_positions.append({
                    "text": entity_text_original,  # S·ª≠ d·ª•ng text g·ªëc (kh√¥ng c√≥ _)
                    "tag": entity_tag,
                    "start": -1,
                    "end": -1
                })

        # G·ªôp c√°c NAME entities li√™n ti·∫øp l·∫°i v·ªõi nhau (post-processing)
        entities_with_positions = self._merge_consecutive_names(entities_with_positions, search_text)
        
        return entities_with_positions

    def _merge_consecutive_names(self, entities: List[Dict[str, any]], text: str) -> List[Dict[str, any]]:
        """
        G·ªôp c√°c NAME entities li√™n ti·∫øp th√†nh m·ªôt entity duy nh·∫•t.
        V√≠ d·ª•: ["Nguy·ªÖn", "VƒÉn", "An"] -> ["Nguy·ªÖn VƒÉn An"]
        
        Args:
            entities: Danh s√°ch entities ƒë√£ c√≥ v·ªã tr√≠
            text: VƒÉn b·∫£n g·ªëc ƒë·ªÉ ki·ªÉm tra
            
        Returns:
            Danh s√°ch entities ƒë√£ ƒë∆∞·ª£c g·ªôp
        """
        if not entities:
            return entities
        
        # S·∫Øp x·∫øp theo v·ªã tr√≠
        sorted_entities = sorted(entities, key=lambda x: (x.get('start', -1), x.get('end', -1)))
        
        merged = []
        i = 0
        
        while i < len(sorted_entities):
            current = sorted_entities[i]
            
            # N·∫øu kh√¥ng ph·∫£i NAME ho·∫∑c kh√¥ng c√≥ v·ªã tr√≠, gi·ªØ nguy√™n
            if current['tag'] != 'NAME' or current.get('start', -1) == -1:
                merged.append(current)
                i += 1
                continue
            
            # T√¨m c√°c NAME entities li√™n ti·∫øp
            consecutive_names = [current]
            j = i + 1
            
            while j < len(sorted_entities):
                next_entity = sorted_entities[j]
                
                # Ki·ªÉm tra xem c√≥ ph·∫£i NAME v√† c√≥ li√™n ti·∫øp kh√¥ng
                if next_entity['tag'] != 'NAME' or next_entity.get('start', -1) == -1:
                    break
                
                # Ki·ªÉm tra kho·∫£ng c√°ch gi·ªØa 2 entities
                gap = next_entity['start'] - consecutive_names[-1]['end']
                
                # N·∫øu kho·∫£ng c√°ch <= 2 k√Ω t·ª± (kho·∫£ng tr·∫Øng, d·∫•u ph·∫©y), coi l√† li√™n ti·∫øp
                if gap <= 2:
                    # Ki·ªÉm tra xem gi·ªØa 2 entities c√≥ g√¨ kh√¥ng
                    between_text = text[consecutive_names[-1]['end']:next_entity['start']]
                    # Ch·ªâ ch·∫•p nh·∫≠n kho·∫£ng tr·∫Øng ho·∫∑c kh√¥ng c√≥ g√¨
                    if between_text.strip() in ['', ',']:
                        consecutive_names.append(next_entity)
                        j += 1
                    else:
                        break
                else:
                    break
            
            # N·∫øu c√≥ nhi·ªÅu h∆°n 1 NAME li√™n ti·∫øp, g·ªôp l·∫°i
            if len(consecutive_names) > 1:
                start_pos = consecutive_names[0]['start']
                end_pos = consecutive_names[-1]['end']
                full_name = text[start_pos:end_pos].strip()
                
                # Lo·∫°i b·ªè d·∫•u ph·∫©y cu·ªëi n·∫øu c√≥
                full_name = full_name.rstrip(',')
                
                merged.append({
                    'text': full_name,
                    'tag': 'NAME',
                    'start': start_pos,
                    'end': start_pos + len(full_name)
                })
            else:
                # Ch·ªâ c√≥ 1 NAME, gi·ªØ nguy√™n (nh∆∞ng lo·∫°i b·ªè d·∫•u ph·∫©y n·∫øu c√≥)
                name_text = current['text'].rstrip(',')
                merged.append({
                    'text': name_text,
                    'tag': 'NAME',
                    'start': current['start'],
                    'end': current['start'] + len(name_text)
                })
            
            i = j
        
        return merged
    
    def _split_sentences(self, text: str) -> List[Dict[str, any]]:
        """
        Chia vƒÉn b·∫£n th√†nh c√°c c√¢u v·ªõi th√¥ng tin offset.

        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o.

        Returns:
            list: Danh s√°ch c√°c dict ch·ª©a {'text': c√¢u, 'start': v·ªã tr√≠ b·∫Øt ƒë·∫ßu, 'end': v·ªã tr√≠ k·∫øt th√∫c}
        """
        # Regex ƒë·ªÉ chia c√¢u theo d·∫•u ch·∫•m, ch·∫•m h·ªèi, ch·∫•m than
        # Gi·ªØ l·∫°i d·∫•u c√¢u
        sentence_pattern = r'([^.!?]*[.!?]+)|([^.!?]+$)'
        matches = re.finditer(sentence_pattern, text)
        
        sentences = []
        for match in matches:
            sentence_text = match.group(0).strip()
            if sentence_text:
                sentences.append({
                    'text': sentence_text,
                    'start': match.start(),
                    'end': match.end()
                })
        
        # N·∫øu kh√¥ng t√¨m th·∫•y c√¢u n√†o, tr·∫£ v·ªÅ to√†n b·ªô text
        if not sentences:
            sentences = [{'text': text, 'start': 0, 'end': len(text)}]
        
        return sentences

    def _create_chunks(self, text: str, max_length: int, overlap: int = 30) -> List[Dict[str, any]]:
        """
        Chia vƒÉn b·∫£n th√†nh c√°c chunks v·ªõi overlap.

        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o.
            max_length (int): ƒê·ªô d√†i t·ªëi ƒëa c·ªßa m·ªói chunk (t√≠nh theo tokens).
            overlap (int): S·ªë tokens overlap gi·ªØa c√°c chunks.

        Returns:
            list: Danh s√°ch c√°c dict ch·ª©a {'text': chunk text, 'start': offset b·∫Øt ƒë·∫ßu}
        """
        # Tokenize to√†n b·ªô vƒÉn b·∫£n
        tokens = self.tokenizer.tokenize(text)
        
        chunks = []
        start_idx = 0
        char_position = 0  # Theo d√µi v·ªã tr√≠ k√Ω t·ª± trong text g·ªëc
        
        while start_idx < len(tokens):
            # L·∫•y chunk t·ª´ start_idx ƒë·∫øn start_idx + max_length
            end_idx = min(start_idx + max_length, len(tokens))
            chunk_tokens = tokens[start_idx:end_idx]
            
            # Convert tokens v·ªÅ text
            chunk_text = self.tokenizer.convert_tokens_to_string(chunk_tokens)
            
            # T√¨m v·ªã tr√≠ ch√≠nh x√°c c·ªßa chunk_text trong text g·ªëc b·∫Øt ƒë·∫ßu t·ª´ char_position
            chunk_start = text.find(chunk_text.strip(), char_position)
            
            # N·∫øu kh√¥ng t√¨m th·∫•y ch√≠nh x√°c, s·ª≠ d·ª•ng char_position hi·ªán t·∫°i
            if chunk_start == -1:
                chunk_start = char_position
            
            chunks.append({
                'text': chunk_text.strip(),
                'start': chunk_start,
                'token_start': start_idx,
                'token_end': end_idx
            })
            
            # C·∫≠p nh·∫≠t char_position cho chunk ti·∫øp theo
            char_position = chunk_start + len(chunk_text.strip())
            
            # Di chuy·ªÉn start_idx, tr·ª´ ƒëi overlap ƒë·ªÉ t·∫°o v√πng ch·ªìng l·∫•n
            if end_idx >= len(tokens):
                break
            start_idx = end_idx - overlap
        
        return chunks

    def _predict_long_text(self, text: str, max_length: int, show_debug: bool = False, 
                          original_text: str = None) -> List[Dict[str, any]]:
        """
        D·ª± ƒëo√°n c√°c th·ª±c th·ªÉ cho vƒÉn b·∫£n d√†i b·∫±ng c√°ch chia th√†nh chunks.

        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o (ƒë√£ ƒë∆∞·ª£c segment).
            max_length (int): ƒê·ªô d√†i t·ªëi ƒëa c·ªßa m·ªói chunk.
            show_debug (bool): Hi·ªÉn th·ªã th√¥ng tin debug hay kh√¥ng.
            original_text (str): VƒÉn b·∫£n g·ªëc (ch∆∞a segment) ƒë·ªÉ t√¨m v·ªã tr√≠ ch√≠nh x√°c.

        Returns:
            list: Danh s√°ch c√°c entity ƒë√£ ƒë∆∞·ª£c g·ªôp v√† lo·∫°i b·ªè tr√πng l·∫∑p.
        """
        # N·∫øu kh√¥ng c√≥ original_text, d√πng text hi·ªán t·∫°i
        if original_text is None:
            original_text = text
        
        # 1. Th·ª≠ chia theo c√¢u tr∆∞·ªõc
        sentences = self._split_sentences(text)
        
        all_entities = []
        
        # 2. X·ª≠ l√Ω t·ª´ng c√¢u ho·∫∑c nh√≥m c√¢u
        current_batch = ""
        current_batch_start = 0
        
        for i, sent_info in enumerate(sentences):
            sentence = sent_info['text']
            sent_tokens = self.tokenizer.tokenize(sentence)
            
            # N·∫øu c√¢u n√†y qu√° d√†i, chia th√†nh chunks
            if len(sent_tokens) > max_length:
                # X·ª≠ l√Ω batch hi·ªán t·∫°i tr∆∞·ªõc (n·∫øu c√≥)
                if current_batch:
                    print(f"  üì¶ Processing batch (offset: {current_batch_start})...")
                    batch_entities = self._predict_single(
                        current_batch, 
                        show_debug=show_debug,
                        original_text=original_text,  # Truy·ªÅn vƒÉn b·∫£n g·ªëc ch∆∞a segment
                        text_offset=current_batch_start
                    )
                    all_entities.extend(batch_entities)
                    current_batch = ""
                
                # Chia c√¢u d√†i th√†nh chunks
                print(f"    Sentence too long ({len(sent_tokens)} tokens) - splitting into chunks...")
                chunks = self._create_chunks(sentence, max_length, overlap=30)
                
                for j, chunk in enumerate(chunks):
                    print(f"     Processing chunk {j+1}/{len(chunks)}...")
                    # T√≠nh offset ch√≠nh x√°c: v·ªã tr√≠ c√¢u trong text + v·ªã tr√≠ chunk trong c√¢u
                    chunk_offset = sent_info['start'] + chunk['start']
                    
                    # Debug info
                    if show_debug:
                        print(f"      Chunk offset: {chunk_offset}")
                        print(f"      Chunk text: {chunk['text'][:50]}...")
                    
                    chunk_entities = self._predict_single(
                        chunk['text'], 
                        show_debug=show_debug,
                        original_text=original_text,  # Truy·ªÅn vƒÉn b·∫£n g·ªëc ch∆∞a segment
                        text_offset=chunk_offset
                    )
                    all_entities.extend(chunk_entities)
                
                current_batch_start = sent_info['end']
            else:
                # Th·ª≠ th√™m c√¢u n√†y v√†o batch
                test_batch = current_batch + " " + sentence if current_batch else sentence
                test_tokens = self.tokenizer.tokenize(test_batch)
                
                if len(test_tokens) <= max_length:
                    # C√≤n ch·ªó, th√™m v√†o batch
                    if not current_batch:
                        current_batch_start = sent_info['start']
                    current_batch = test_batch
                else:
                    # Batch ƒë·∫ßy, x·ª≠ l√Ω batch hi·ªán t·∫°i
                    if current_batch:
                        print(f"   Processing batch (offset: {current_batch_start})...")
                        batch_entities = self._predict_single(
                            current_batch, 
                            show_debug=show_debug,
                            original_text=original_text,  # Truy·ªÅn vƒÉn b·∫£n g·ªëc
                            text_offset=current_batch_start
                        )
                        all_entities.extend(batch_entities)
                    
                    # B·∫Øt ƒë·∫ßu batch m·ªõi v·ªõi c√¢u hi·ªán t·∫°i
                    current_batch = sentence
                    current_batch_start = sent_info['start']
        
        # X·ª≠ l√Ω batch cu·ªëi c√πng
        if current_batch:
            print(f"   Processing final batch (offset: {current_batch_start})...")
            batch_entities = self._predict_single(
                current_batch, 
                show_debug=show_debug,
                original_text=original_text,  # Truy·ªÅn vƒÉn b·∫£n g·ªëc
                text_offset=current_batch_start
            )
            all_entities.extend(batch_entities)
        
        # 3. Lo·∫°i b·ªè entities tr√πng l·∫∑p (t·ª´ v√πng overlap)
        unique_entities = self._remove_duplicates(all_entities)
        
        print(f" Completed! Found {len(unique_entities)} unique entities.\n")
        
        return unique_entities

    def _remove_duplicates(self, entities: List[Dict[str, any]]) -> List[Dict[str, any]]:
        """
        Lo·∫°i b·ªè c√°c entities tr√πng l·∫∑p d·ª±a tr√™n v·ªã tr√≠ v√† text.

        Args:
            entities (list): Danh s√°ch c√°c entities.

        Returns:
            list: Danh s√°ch entities ƒë√£ lo·∫°i b·ªè tr√πng l·∫∑p.
        """
        if not entities:
            return []
        
        # S·∫Øp x·∫øp theo v·ªã tr√≠ start
        sorted_entities = sorted(entities, key=lambda x: (x.get('start', -1), x.get('end', -1)))
        
        unique = []
        seen = set()
        
        for entity in sorted_entities:
            # T·∫°o key duy nh·∫•t cho entity
            key = (entity['text'].strip(), entity['tag'], entity.get('start', -1))
            
            if key not in seen:
                seen.add(key)
                unique.append(entity)
        
        return unique

def main():
    """H√†m main ƒë·ªÉ demo c√°ch s·ª≠ d·ª•ng class NERPredictor."""
    print("--- Demo NER Prediction ---")
    
    # Kh·ªüi t·∫°o predictor
    predictor = NERPredictor(model_path=config.MODEL_OUTPUT_DIR)

    # N·∫øu model ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng
    if predictor.model:
        # C√¢u v√≠ d·ª•
        sentence1 = "B·ªánh nh√¢n Nguy·ªÖn VƒÉn An, 50 tu·ªïi, nh·∫≠p vi·ªán t·∫°i B·ªánh vi·ªán B·∫°ch Mai v·ªõi tri·ªáu ch·ª©ng s·ªët cao."
        sentence2 = "B√† Tr·∫ßn Th·ªã B qu√™ ·ªü H√† N·ªôi, l√†m ngh·ªÅ gi√°o vi√™n."

        print(f"\nC√¢u 1: '{sentence1}'")
        entities1 = predictor.predict(sentence1)
        print("C√°c th·ª±c th·ªÉ ƒë∆∞·ª£c nh·∫≠n d·∫°ng:")
        print(entities1)

        print(f"\nC√¢u 2: '{sentence2}'")
        entities2 = predictor.predict(sentence2)
        print("C√°c th·ª±c th·ªÉ ƒë∆∞·ª£c nh·∫≠n d·∫°ng:")
        print(entities2)

if __name__ == "__main__":
    main()

